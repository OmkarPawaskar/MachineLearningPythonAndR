**Machine learning may seem intimidating at first, but the entire field is just many simple ideas combined together to yield extremely accurate models that can ‘learn’ from past data.**
_______________________________________________________________________________________________________________________________________________________________

REGRESSION

Difference between Python and R: indexing start from 0 in python while it starts from 1 in R

iloc -> integer-location based indexing for selection by position.
Imputer #for completing missing values
LabelEncoder,OneHotEncoder #LabelEncoder to encode values and OneHotEncoder to give dummy values

random_state simply sets a seed to the random generator, so that your train-test splits are always deterministic. If you don't set a seed, it is different each time.
#This ensures that the random numbers are generated in the same order

difference between scatter and plot 
-scatter draws points without lines connecting them whereas plot may or may not plot the lines, depending on the arguments.
-scatter allows you to specify a different colour and a different size for each point individually. 
It is not possible to do that with plot.

decision tree :
http://www.simafore.com/blog/bid/62333/4-key-advantages-of-using-decision-trees-for-predictive-analytics
advantages: 1) perform variable screening or feature selection
	    2) require relatively little effort from users for data preparation	
	    3) Nonlinear relationships between parameters do not affect tree performance
	    4) The best feature of using trees for analytics - easy to interpret and explain to executives!

disadvantages: overfitting - high variance classifiers i.e. the DT learnt is sensitive to the precise layout of points and, 
if you have less data, can fit to noise. 
Of course, high-variance of DTs can be addressed, and is usually done so with ensembling.

random forest = group of decision trees
The fundamental idea behind a random forest is to combine many decision trees into a single model. 
Individually, predictions made by decision trees (or humans) may not be accurate, 
but combined together, the predictions will be closer to the mark on average.
-each decision tree in the forest considers a random subset of features when forming questions and only has access to a random set of the training data points.

-one of the biggest advantages of using Decision Trees and Random Forests is,
the ease in which we can see what features or variables contribute to the classification or regression and their relative importance based on their location depthwise in the tree.

Ensemble learning - Ensemble Learning is when you take multiple algorithms or same algorithms multiple times and you put them together
to make something much more powerful than the original.Random forest is part of Ensemble Learning.


EVALUATING THE REGRESSION MODEL PERFORMANCE

1) R-SQUARED(Simple Linear Regression) - Goodness of fit
	SS(res) = sum(y - h(x)).^2
	SS(total) = sum(y - h(x)(avg)).^2
	R^2(R-square) = 1 - SS(res)/SS(total)
	
	value of R-square should be closer to 1.Ideally it is hardly 1 .value of R-square can be negative.

2)Adjusted-R-Squared(Multiple Linear Regression)
	problem with R-square-> in example of salary based on experience where salary is dependent variable(y) and experience is independent
variable(x) ,if we add another independent variable 'Mobile Number' which has no correlation with y .but it will still affect the model.
hence Adjusted R-Squared Model is used.

	Adj R^2 = 1 - ((1-R^2)*((n-1)/(n-p-1)))
	p = number of regressors
	n = sample size	

- as in Multiple Linear Regression.py during backward elimination ,we eliminated 'Marketing.Spend' because of p value of 0.6 when significance level
was 0.5.BUT in such cases,check the value of Adjusted R-Squared,if the value is increasing with elimination of predictors,it is good thing.but if it decreases means model 
is bad.Hence when we eliminated 'Marketing.Spend' cause of p value even though it was close,if we check adjusted r^2 value it decreases .
Hence 'Marketing.Spend' shouldnt be eliminated for better performance.

_______________________________________________________________________________________________________________________________________________________________

CLASSIFICATION

Logistic Regression - It’s a classification algorithm, that is used where the response variable is categorical. 
The idea of Logistic Regression is to find a relationship between features and probability of particular outcome.
-You may be wondering why the name says regression if it is a classification algorithm, well,**It uses the regression inside to be the classification algorithm.**

Logistic Regression - Linear Classifier
KNN - Non Linear Classifier
SVM - linear classifier
kernel SVM - Gaussian Kernel Classifier


Kernel SVM - when data is not linearly seperable we use kernel SVM.Kernel SVM changes the data from 2D to 3D to data is now linearly seperable.
disadvantage: Mapping to higher dimensional space can be highly compute-inclusive.

Naive Bayes - Collection of classification algorithms based on Bayes Theorem.
Classifies given different instances (object/data) into predefined classes(groups), assuming there is no interdependency of features
(class conditional independence). ie * assuming features are independent of each other
eg:
Since, there are a total of 60 objects, 40 of which are GREEN and 20 RED, our prior probabilities for class membership are:

Prior Probability of GREEN: number of GREEN objects / total number of objects = 40 / 60

Prior Probability of RED: number of RED objects / total number of objects = 20 / 60

From the illustration above, it is clear that Likelihood of X given GREEN is smaller than Likelihood of X given RED, since the circle encompasses 1GREEN object and 3RED ones. 
In the Bayesian analysis, the final classification is produced by combining both sources of information, 
i.e., the prior and the likelihood, to form a posterior probability using the so-called Bayes' rule.
posterior probability = prio probability X likelihood


Decision Tree Classifier - Decision Tree Classifier, repetitively divides the working area(plot) into sub part by identifying lines. 
(repetitively because there may be two distant regions of same class divided by other ).
NOTE - Dividing efficiently based on maximum information gain is key to decision tree classifier. 
However, in real world with millions of data dividing into pure class in practically not feasible (it may take longer training time) and 
so we stop at points in nodes of tree when fulfilled with certain parameters (for example impurity percentage)
https://medium.com/machine-learning-101/chapter-3-decision-trees-theory-e7398adac567

Random Forest Classifier-
Random forest classifier creates a set of decision trees from randomly selected subset of training set. 
It then aggregates the votes from different decision trees to decide the final class of the test object.
This works well because a single decision tree may be prone to a noise, but aggregate of many decision trees reduce the effect of noise giving more accurate results.

Criterion in Decision tree and Random Forest
Gini Impurity vs Entropy - 
Gini impurity and Information Gain Entropy are pretty much the same. And people do use the values interchangeably. Below are the formulae of both:

Gini:Gini(E)=1-?cj=1p2j
Entropy:H(E)=-?cj=1pjlogpj
Gini is intended for continuous attributes and Entropy is for attributes that occur in classes

Gini is to minimize misclassification
Entropy is for exploratory analysis


Evaluating Classification Models Performance- 
1. Confusion Matrix
2. Cumulative Accuracy Profile (CAP) Curve - The better your model, the larger will be the area between its CAP curve and the random scenario straight line.
https://medium.com/@lotass/classification-models-performance-evaluation-c3a91562793


_______________________________________________________________________________________________________________________________________________________________

CLUSTERING : 
-unsupervised learning
-Cluster is the collection of data objects which are similar to one another within the same group (class or category) and are different from the objects in the other clusters.
-Why Clustering?

Clustering allows us to find hidden relationship between the data points in the dataset.
2 types:
1. Flat or partitioning algorithms - K-means

2. Hierarchical algorithms:
	a)Agglomerative(bottom to top)
	b)Divisive(top to bottom)

To find optimal no. of clusters :
1)Elbow method (KMeans) 
2) Dendogram Method - finding longest vertical line which is not intersected by horizontal line .AND then passing hypothetical horizontal line
through it to find number of clusters.(check intuition in videos) 

_______________________________________________________________________________________________________________________________________________________________

ASSOCIATION RULE LEARNING:

Mainly used as Market Basket Analysis (which products will be bought together - eg bread and milk)

1.Apriori
support is P(XY), confidence is P(XY)/P(X) and lift is P(XY)/P(X)P(Y), where the lift is a measurement of independence of X and Y (1 represents independent)
high support: should apply to a large amount of cases
high confidence: should be correct often
high lift: indicates it is not just a coincidence
2.Eclat-can also be called simplified version of Apriori
-only based on support
-To optimize the sales and revenue -> Apriori
-but for simple information :set of products purchased together -> Eclat

_______________________________________________________________________________________________________________________________________________________________

REINFORCEMENT LEARNING
Reinforcement Learning is a type of Machine Learning, and thereby also a branch of Artificial Intelligence. 
It allows machines and software agents to automatically determine the ideal behaviour within a specific context, in order to maximize its performance.

1) Upper Bound Confidence Strategy 
 We always pick the option with the highest possible outcome, even if it that outcome very unlikely. 
The intuition behind this strategy is that options with high uncertainty usually lead to a lot of new knowledge. 
We don’t know enough about the option to accurately estimate the return and by pursuing that option we are bound to learn more and improve our future estimations. 
For example, joining a startup could yield $0 of profits or $1B of profits. 
Since most startups fail the expected outcome would probably be much closer to $0 than $1B, but the chance of making $1B still exists.

2)Thompson Sampling

_______________________________________________________________________________________________________________________________________________________________

Natural Language Processing

https://medium.com/@ageitgey/natural-language-processing-is-fun-9a0bff37854e 
check diff steps of nlp.

Applications:
You can also use NLP on a text review to predict if the review is a good one or a bad one. 
You can use NLP on an article to predict somecategories of the articlesyou are trying to segment.
You can use NLP on a book to predict the genre of the book.
And it can go further, you can use NLP to build a machine translator or a speech recognition system, and int hat last example you use classification algorithms to classify language.
Speaking of classification algorithms, most of NLP algorithms are classification models, and they include Logistic Regression,
A very well-known model in NLP is the Bagof Words model. It is a model used to preprocess the texts to classify before fitting the classification algorithms on the observations containing the texts.

Stemming and Lemmatization:
https://blog.bitext.com/what-is-the-difference-between-stemming-and-lemmatization/

Bag of words- 
https://machinelearningmastery.com/gentle-introduction-bag-words-model/

Bag of Words (BoW) model is a simple algorithm used in Natural Language Processing.
In BoW model a sentence or a document is considered as a 'Bag' containing words. 
It will take into account the words and their frequency of occurrence in the sentence or the document disregarding semantic relationship in the sentences.

Sparse Matrix - Sparse matrix is a matrix which contains very few non-zero elements. 
When a sparse matrix is represented with 2-dimensional array, we waste lot of space to represent that matrix. 
For example, consider a matrix of size 100 X 100 containing only 10 non-zero elements.

_______________________________________________________________________________________________________________________________________________________________

Deep Learning

Deep Learning is the most exciting andpowerful branch ofMachine Learning. Deep Learning models can be used for a variety of complex tasks:

-Artificial Neural Networksfor Regression and Classification
-Convolutional Neural Networks for Computer Vision
-Recurrent Neural Networks for Time Series Analysis
-Self Organizing Maps for Feature Extraction
-Deep Boltzmann Machinesfor Recommendation Systems
-Auto Encoders for Recommendation Systems


Artificial Neural Networks(ANN):
https://medium.com/@tharanignanasegaram/artificial-neural-network-a-brief-introduction-572d462666f1
https://medium.com/machinevision/overview-of-neural-networks-b86ce02ea3d1

Artificial Neural Network (ANN) is a computational model used in Machine Learning which works similar to biological neurons.
- Activation fucnction - 1.Threshold Function 2.Sigmoid Function 3.Rectifier Function(most used in ANN) 4.Tangential Function

Cost Function - A cost function is a measure of "how good" a neural network did with respect to it's given training sample and the expected output. 
It also may depend on variables such as weights and biases.