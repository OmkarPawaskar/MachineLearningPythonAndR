**Machine learning may seem intimidating at first, but the entire field is just many simple ideas combined together to yield extremely accurate models that can ‘learn’ from past data.**
_______________________________________________________________________________________________________________________________________________________________

REGRESSION

Difference between Python and R: indexing start from 0 in python while it starts from 1 in R

iloc -> integer-location based indexing for selection by position.
Imputer #for completing missing values
LabelEncoder,OneHotEncoder #LabelEncoder to encode values and OneHotEncoder to give dummy values

random_state simply sets a seed to the random generator, so that your train-test splits are always deterministic. If you don't set a seed, it is different each time.
#This ensures that the random numbers are generated in the same order

difference between scatter and plot 
-scatter draws points without lines connecting them whereas plot may or may not plot the lines, depending on the arguments.
-scatter allows you to specify a different colour and a different size for each point individually. 
It is not possible to do that with plot.

decision tree :
http://www.simafore.com/blog/bid/62333/4-key-advantages-of-using-decision-trees-for-predictive-analytics
advantages: 1) perform variable screening or feature selection
	    2) require relatively little effort from users for data preparation	
	    3) Nonlinear relationships between parameters do not affect tree performance
	    4) The best feature of using trees for analytics - easy to interpret and explain to executives!

disadvantages: overfitting - high variance classifiers i.e. the DT learnt is sensitive to the precise layout of points and, 
if you have less data, can fit to noise. 
Of course, high-variance of DTs can be addressed, and is usually done so with ensembling.

random forest = group of decision trees
The fundamental idea behind a random forest is to combine many decision trees into a single model. 
Individually, predictions made by decision trees (or humans) may not be accurate, 
but combined together, the predictions will be closer to the mark on average.
-each decision tree in the forest considers a random subset of features when forming questions and only has access to a random set of the training data points.

-one of the biggest advantages of using Decision Trees and Random Forests is,
the ease in which we can see what features or variables contribute to the classification or regression and their relative importance based on their location depthwise in the tree.

Ensemble learning - Ensemble Learning is when you take multiple algorithms or same algorithms multiple times and you put them together
to make something much more powerful than the original.Random forest is part of Ensemble Learning.


EVALUATING THE REGRESSION MODEL PERFORMANCE

1) R-SQUARED(Simple Linear Regression) - Goodness of fit
	SS(res) = sum(y - h(x)).^2
	SS(total) = sum(y - h(x)(avg)).^2
	R^2(R-square) = 1 - SS(res)/SS(total)
	
	value of R-square should be closer to 1.Ideally it is hardly 1 .value of R-square can be negative.

2)Adjusted-R-Squared(Multiple Linear Regression)
	problem with R-square-> in example of salary based on experience where salary is dependent variable(y) and experience is independent
variable(x) ,if we add another independent variable 'Mobile Number' which has no correlation with y .but it will still affect the model.
hence Adjusted R-Squared Model is used.

	Adj R^2 = 1 - ((1-R^2)*((n-1)/(n-p-1)))
	p = number of regressors
	n = sample size	

- as in Multiple Linear Regression.py during backward elimination ,we eliminated 'Marketing.Spend' because of p value of 0.6 when significance level
was 0.5.BUT in such cases,check the value of Adjusted R-Squared,if the value is increasing with elimination of predictors,it is good thing.but if it decreases means model 
is bad.Hence when we eliminated 'Marketing.Spend' cause of p value even though it was close,if we check adjusted r^2 value it decreases .
Hence 'Marketing.Spend' shouldnt be eliminated for better performance.

_______________________________________________________________________________________________________________________________________________________________

CLASSIFICATION

Logistic Regression - It’s a classification algorithm, that is used where the response variable is categorical. 
The idea of Logistic Regression is to find a relationship between features and probability of particular outcome.
-You may be wondering why the name says regression if it is a classification algorithm, well,**It uses the regression inside to be the classification algorithm.**

Logistic Regression - Linear Classifier
KNN - Non Linear Classifier
SVM - linear classifier
kernel SVM - Gaussian Kernel Classifier


Kernel SVM - when data is not linearly seperable we use kernel SVM.Kernel SVM changes the data from 2D to 3D to data is now linearly seperable.
disadvantage: Mapping to higher dimensional space can be highly compute-inclusive.

Naive Bayes - Collection of classification algorithms based on Bayes Theorem.
Classifies given different instances (object/data) into predefined classes(groups), assuming there is no interdependency of features
(class conditional independence). ie * assuming features are independent of each other
eg:
Since, there are a total of 60 objects, 40 of which are GREEN and 20 RED, our prior probabilities for class membership are:

Prior Probability of GREEN: number of GREEN objects / total number of objects = 40 / 60

Prior Probability of RED: number of RED objects / total number of objects = 20 / 60

From the illustration above, it is clear that Likelihood of X given GREEN is smaller than Likelihood of X given RED, since the circle encompasses 1GREEN object and 3RED ones. 
In the Bayesian analysis, the final classification is produced by combining both sources of information, 
i.e., the prior and the likelihood, to form a posterior probability using the so-called Bayes' rule.
posterior probability = prio probability X likelihood


Decision Tree Classifier - Decision Tree Classifier, repetitively divides the working area(plot) into sub part by identifying lines. 
(repetitively because there may be two distant regions of same class divided by other ).
NOTE - Dividing efficiently based on maximum information gain is key to decision tree classifier. 
However, in real world with millions of data dividing into pure class in practically not feasible (it may take longer training time) and 
so we stop at points in nodes of tree when fulfilled with certain parameters (for example impurity percentage)
https://medium.com/machine-learning-101/chapter-3-decision-trees-theory-e7398adac567

Random Forest Classifier-
Random forest classifier creates a set of decision trees from randomly selected subset of training set. 
It then aggregates the votes from different decision trees to decide the final class of the test object.
This works well because a single decision tree may be prone to a noise, but aggregate of many decision trees reduce the effect of noise giving more accurate results.

Criterion in Decision tree and Random Forest
Gini Impurity vs Entropy - 
Gini impurity and Information Gain Entropy are pretty much the same. And people do use the values interchangeably. Below are the formulae of both:

Gini:Gini(E)=1-?cj=1p2j
Entropy:H(E)=-?cj=1pjlogpj
Gini is intended for continuous attributes and Entropy is for attributes that occur in classes

Gini is to minimize misclassification
Entropy is for exploratory analysis

